{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bytenet_blank.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAu7iOsOCs_k",
        "colab_type": "text"
      },
      "source": [
        "In this task you will implement ByteNet decoder https://arxiv.org/abs/1610.10099 for a language modeling task. Do to this you will write a custom layer for masked convolutions and define a residual blocks used in the architecture.\n",
        "\n",
        "We will use the Hutter Prize dataset (https://en.wikipedia.org/wiki/Hutter_Prize, http://prize.hutter1.net/) that contains 100 megabytes of English wikipedia pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39QYTrEXCs_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from torch import nn\n",
        "\n",
        "disable_cuda = True\n",
        "\n",
        "if not disable_cuda and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nZc79qCs_v",
        "colab_type": "text"
      },
      "source": [
        "# ByteNet Architecture\n",
        "\n",
        "**Task 1.** Define a masked convolution layer. This layer must multiply the convolutional filter with a binary mask that prohibits the network from looking into the subsequent tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ3qPxq0Cs_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedConv1d(nn.Conv1d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MaskedConv1d, self).__init__(*args, **kwargs)\n",
        "        # TODO: add mask\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: multiply the weights\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5c1aHF7Cs_0",
        "colab_type": "text"
      },
      "source": [
        "If the layer implementation is correct, for kernel size 3 the gradient of the first output wrt input must be non-zero only for the first two tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0aGKH0wCs_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input = torch.rand(1, 3, 7, requires_grad=True)\n",
        "masked_layer = MaskedConv1d(in_channels=3, out_channels=1, kernel_size=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxocDDUjCs_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if test_input.grad is not None:\n",
        "    test_input.grad.data.zero_()\n",
        "test_output = masked_layer(test_input)\n",
        "test_output[0, 0, 0].backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2aTj8B4MCs_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_input.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQpQA5hiCs__",
        "colab_type": "text"
      },
      "source": [
        "The default implementation of layernorm normalizes wrt last dimensions. When we use convolutional architectures, the last dimension is time and the default implementation of LayerNorm can lead to leakage of information about the future tokens. \n",
        "\n",
        "To avoid this, we modify LayerNorm to normalize wrt the channel dimension (the last but one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT1KNX4gCtAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModifiedLayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ModifiedLayerNorm, self).__init__(*args, **kwargs)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = super(ModifiedLayerNorm, self).forward(x.transpose(2, 1))\n",
        "        return output.transpose(2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzzx62WSCtAD",
        "colab_type": "text"
      },
      "source": [
        "**Task 2.** Use the masked convolution layer and the modified layer norm to define the residual block depicted in https://arxiv.org/abs/1610.10099 Figre 3 on the left.\n",
        "\n",
        "Pay attention to padding parameter of masked convolution layer. Choose the padding parameter to preserve the length of the input sequence. Note that the correct padding parameter depends on the kernel size and the dilation parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHx-PcMVCtAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d, dilation, kernel_size):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # TODO\n",
        "        pass\n",
        "        \n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab_lbaezCtAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input = b = torch.rand(1, 6, 17)\n",
        "res_block = ResidualBlock(d=3, dilation=3, kernel_size=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PKTBZjPHCtAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the output must be 17 no matter what dilation and kernel_size we choose\n",
        "res_block(test_input).size(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1GEXLhCtAN",
        "colab_type": "text"
      },
      "source": [
        "Now we combine the residual blocks into the decoder architecture. Section five of https://arxiv.org/abs/1610.10099 specifies the details of the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlNiTKwVCtAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ByteNetDecoder(nn.Module):\n",
        "    def __init__(self, n_symbols, d, kernel_size, dilations, groupsize):\n",
        "        super(ByteNetDecoder, self).__init__()\n",
        "        dilations_in_groups = [d for d in dilations[::-1] for i in range(groupsize)]\n",
        "        self.embedding = nn.Embedding(n_symbols, 2 * d) # to encode tokents into vectors of suitable dimensionality\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(d, dilation, kernel_size)\n",
        "              for dilation in dilations_in_groups],\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv1d(2 * d, n_symbols, 1))\n",
        "\n",
        "        self.d = d\n",
        "        self.n_symbols = n_symbols\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilations = dilations\n",
        "        self.groupsize = groupsize\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_length = x.size(-1)\n",
        "        x_embedded = self.embedding(x)\n",
        "        x_embedded = x_embedded.transpose(-1, -2).view(-1, 2 * self.d, x_length)\n",
        "        bytenet = self.residual_blocks(x_embedded)\n",
        "        return bytenet.transpose(-1, -2).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9JB_2dfCtAQ",
        "colab_type": "text"
      },
      "source": [
        "In particular, the decoder stacks several residual blocks with fixed dilation parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGi-5V5LCtAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groupsize = 5 # number of residual blocks in a group\n",
        "dilations = [1, 2, 4, 8, 16] # dilation sizes within each group\n",
        "dilations_in_groups = [d for d in dilations[::-1] for i in range(groupsize)]\n",
        "print(dilations_in_groups)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpffTOvBCtAT",
        "colab_type": "text"
      },
      "source": [
        "In total, the model uses 25 residual blocks with 3 convolutional layers within each block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqqgYv2eCtAU",
        "colab_type": "text"
      },
      "source": [
        "# Applying the model to Hutter Prize dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgTfib3YCtAU",
        "colab_type": "raw"
      },
      "source": [
        "Now the notebook assumes that you have an unpacked enwik8.zip file in the folder. You can download and unpack the data with the following commands:\n",
        "!wget http://mattmahoney.net/dc/enwik8.zip\n",
        "!unzip enwik8.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_rEUDU4CtAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HutterPrizeDataset():\n",
        "    def __init__(self, filename, device):\n",
        "        self.f, self.symbols = self._getFileAndUniqueSymbols(filename)\n",
        "        # this split is typically considered in the literature\n",
        "        self.ranges = {\n",
        "            'train' : (0, math.floor(0.90 * len(self.f))),\n",
        "            'validation': (math.floor(0.90 * len(self.f)),\n",
        "                           math.floor(0.95 * len(self.f))),\n",
        "            'test': (math.floor(0.95 * len(self.f)), len(self.f))}\n",
        "        self.device = device\n",
        "\n",
        "    def _getFileAndUniqueSymbols(self, filename):\n",
        "        # In utf-8 encoding there are 6064 unique symbols, but in iso-8859-1 there are 205\n",
        "        f = open(filename, 'r', encoding='iso-8859-1').read()\n",
        "        unique_symbols = ''.join(sorted(set(f)))\n",
        "        return f, unique_symbols\n",
        "\n",
        "    def intEncoding(self, text):\n",
        "        for i, t in enumerate(text):\n",
        "            if self.symbols.find(t) is -1:\n",
        "                raise NameError('Cannot process the string. '\n",
        "                                'Symbol \"%s\" is not in the dataset' % t)\n",
        "            self.encoding_tensor[i] = self.symbols.find(t)\n",
        "        return self.encoding_tensor\n",
        "\n",
        "    def intDecoding(self, tensor):\n",
        "        text = ''\n",
        "        for t in tensor:\n",
        "            text += self.symbols[t]\n",
        "        return text\n",
        "\n",
        "    def iterate_batches(self, batchsize, type='train', encoding=True):\n",
        "        n_steps = (self.ranges[type][1] - self.ranges[type][0]) // batchsize\n",
        "        self.encoding_tensor = torch.zeros(batchsize, dtype=torch.long, device=self.device)\n",
        "        for n in range(n_steps):\n",
        "            if encoding:\n",
        "                yield self.intEncoding(self.f[\n",
        "                    self.ranges[type][0] + batchsize * n:\n",
        "                    self.ranges[type][0] + batchsize * (n + 1)])\n",
        "            else:\n",
        "                yield self.f[self.ranges[type][0] + batchsize * n:\n",
        "                             self.ranges[type][0] + batchsize * (n + 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9mlGlT4CtAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = HutterPrizeDataset('enwik8', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SqeR_b3CtAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to simplify computation you can reduce width or depth of the network\n",
        "decoder = ByteNetDecoder(\n",
        "    n_symbols=len(data.symbols),\n",
        "    d=512, # controls width\n",
        "    kernel_size=3,\n",
        "    dilations=[1, 2, 4, 8, 16],\n",
        "    groupsize=5) # controls depth\n",
        "decoder.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZKQHsPCtAb",
        "colab_type": "text"
      },
      "source": [
        "Try running the decoder on an arbitrary input to debug you implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VPcnDO2CtAc",
        "colab_type": "text"
      },
      "source": [
        "# Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytpmu_TqCtAd",
        "colab_type": "text"
      },
      "source": [
        "If you impelementation is correct, the following script will train a ByteNet decoder. Try running it for several iterations.\n",
        "\n",
        "- In practice, you should remember to shift the targets by one for during training.\n",
        "- Note that we do not use the first 100 outputs during training. Why?\n",
        "- The training procedure uses batch of size 1\n",
        "- At every iteration the network updates the predictions for 400 symbols in the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqeZpTz1CtAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainDecoder(decoder, dataset, n_epochs=4):\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(decoder.parameters(),\n",
        "                                 lr=0.0003,\n",
        "                                 weight_decay=1e-4)\n",
        "    for epoch in range(n_epochs):\n",
        "        decoder.train()\n",
        "        running_train_loss = 0.\n",
        "        n_its = (dataset.ranges['train'][1] - dataset.ranges['train'][0]) // 500\n",
        "        print('Epoch #%d \\tTotal number of iterations: %d' % (epoch, n_its))\n",
        "        train_iterator = tqdm(enumerate(dataset.iterate_batches(500, 'train')))\n",
        "        for t, batch in train_iterator:\n",
        "            optimizer.zero_grad()\n",
        "            target = batch[100:]\n",
        "            logits = decoder(batch)[99:-1]\n",
        "            loss = criterion(logits, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += (loss.item() - running_train_loss) / (t + 1)\n",
        "            train_iterator.set_postfix(loss=running_train_loss)            \n",
        "    \n",
        "        train_losses.append(running_train_loss)\n",
        "\n",
        "        n_its = (dataset.ranges['validation'][1] - dataset.ranges['validation'][0]) // 500\n",
        "        print('Validation \\t Total number of iterations: %d' % n_its)\n",
        "        decoder.eval()\n",
        "        running_val_loss = 0.\n",
        "        val_iterator = tqdm(enumerate(dataset.iterate_batches(500, 'validation')))\n",
        "        for t, batch in val_iterator:\n",
        "            target = batch[100:]\n",
        "            logits = decoder(batch)[100:]\n",
        "            loss = criterion(logits, target)\n",
        "            running_val_loss += (loss.item() - running_val_loss) / (t + 1)\n",
        "            val_iterator.set_postfix(loss=running_val_loss)\n",
        "            \n",
        "        torch.save(decoder.state_dict(), 'checkpoint_epoch_%d' % epoch + 1)\n",
        "            \n",
        "        validation_losses.append(running_val_loss)\n",
        "\n",
        "    return train_losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VYdQEMbCtAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainDecoder(decoder, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqqNjmR8CtAm",
        "colab_type": "text"
      },
      "source": [
        "** Task 3. ** Implement a function that samples the next character given the condition $x$. Train the model for several epochs and generate some samples using the trained model. If everything works properly, validation loss will be in [1., 2.]. Try extracts from the test set and arbitrary text as a condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYDu00cxCtAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_next_symbol(decoder, x, T=1.):\n",
        "    # TODO\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1kPHqsXCtAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuob7aC8CtAy",
        "colab_type": "text"
      },
      "source": [
        "** Optional task 4. ** In the original papers the authors used a different block for language modeling. Implement the MU block from Figure 3. Compare the performance of the basic block and the MU block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWSrLuYZCtAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}