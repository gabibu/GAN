{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_dcgan_blank.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-iHk6_wOR0r",
        "colab_type": "text"
      },
      "source": [
        "# DCGAN on CIFAR-10\n",
        "\n",
        "In this assignment, you are to train a DCGAN on CIFAR-10. Below you may find the data loader and the generator and discriminator networks. The assignment consists of the following tasks:\n",
        "\n",
        "**1)**  Compare the two ways to update generator discussed in the original [paper](https://arxiv.org/abs/1406.2661) (page 3, above the figure). The first is two minimise $\\mathbb E_z \\log (1 - D(G(z))$ and the second is to maximise $\\mathbb E_z \\log D(G(z))$.\n",
        "\n",
        "To do this fix a small number of training epochs (around 5) and plot the values of generator and discriminator losses for both cases. How do the two graphs compare? As a sanity check, plot the fake samples at the end of each epoch.\n",
        "\n",
        "**2)** Train a GAN with the second generator update and a more significant number of training epochs. To track the training progress plot fake samples at the end of each epoch. How do the resulting samples compare with the real data samples visually?\n",
        "\n",
        "**3)** To estimate the generalization properties of the generator qualitatively, plot interpolations between randomly generated samples. To do this, pick two random noise vectors, connect them with a line and decode the points on the line using the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gam4AtwOR0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1TpTZfBOR0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = './data'\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "n_channels = 3\n",
        "n_feature_maps = 64\n",
        "z_dim = 128\n",
        "\n",
        "# for the data to be compatible with the DCGAN architecture we will resize it to 64*64\n",
        "dataset = torchvision.datasets.CIFAR10(\n",
        "    root=data_folder,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]))\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e577jST7OR04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dcgan_generator(z_dim, n_feature_maps=64, n_channels=3):\n",
        "    layers = [\n",
        "        nn.ConvTranspose2d(z_dim, n_feature_maps * 8, 4, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 8),\n",
        "        nn.ReLU(),\n",
        "        # n_features * 8 x 4 x 4\n",
        "        nn.ConvTranspose2d(n_feature_maps * 8, n_feature_maps * 4, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 4),\n",
        "        nn.ReLU(),\n",
        "        # n_features * 4 x 8 x 8\n",
        "        nn.ConvTranspose2d(n_feature_maps * 4, n_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 2),\n",
        "        nn.ReLU(),\n",
        "        # n_features * 2 x 16 x 16\n",
        "        nn.ConvTranspose2d(n_feature_maps * 2, n_feature_maps, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps),\n",
        "        nn.ReLU(),\n",
        "        # n_features x 32 x 32\n",
        "        nn.ConvTranspose2d(n_feature_maps, n_channels, 4, 2, 1, bias=False),\n",
        "        nn.Tanh()]\n",
        "        # n_channels x 64 x 64\n",
        "    return nn.Sequential(*layers)\n",
        "    \n",
        "def get_dcgan_discriminator(n_feature_maps=64, n_channels=3):\n",
        "    layers = [\n",
        "        nn.Conv2d(n_channels, n_feature_maps, 4, 2, 1, bias=False),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        # n_feature_maps x 32 x 32\n",
        "        nn.Conv2d(n_feature_maps, n_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        # n_feature_maps * 2 x 16 x 16\n",
        "        nn.Conv2d(n_feature_maps * 2, n_feature_maps * 4, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        # n_feature_maps * 4 x 8 x 8\n",
        "        nn.Conv2d(n_feature_maps * 4, n_feature_maps * 8, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_feature_maps * 8),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        # n_feature_maps * 8 x 4 x 4\n",
        "        nn.Conv2d(n_feature_maps * 8, 1, 4, 1, 0, bias=False),\n",
        "        nn.Sigmoid()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK7DyVAEOR07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = get_dcgan_generator(z_dim).to(device)\n",
        "generator.apply(weights_init)\n",
        "discriminator = get_dcgan_discriminator().to(device)\n",
        "discriminator.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGKGOo9OR0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002,\n",
        "                         betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(),\n",
        "                         lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZgbT-vdOR1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2SHqlEsOR1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # update discriminator\n",
        "        discriminator.zero_grad()\n",
        "        real_data = data[0].to(device)\n",
        "        d_real_scores = discriminator(real_data)\n",
        "        \n",
        "        noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
        "        fake_samples = generator(noise)\n",
        "        d_fake_scores = discriminator(fake_samples)\n",
        "        # TODO\n",
        "        \n",
        "        # update generator\n",
        "        generator.zero_grad()\n",
        "        noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
        "        fake_samples = generator(noise)\n",
        "        d_fake_scores = discriminator(fake_samples)\n",
        "        # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcp7Arg0OR1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}